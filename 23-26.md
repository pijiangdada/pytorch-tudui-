# p23 损失函数与反向传播



1、就是实际预测值和目标值之间的差的关系（可能是差，也可能是均差、均方误差....）（都是越小越好）

![image-20220918085238232](typora-user-images\image-20220918085238232.png)

2、查看官方文档

[torch.nn — PyTorch 1.8.1 documentation](https://pytorch.org/docs/1.8.1/nn.html)

![image-20220918085416934](typora-user-images\image-20220918085416934.png)

## 1 nn.L1Loss() : 差值的和的平均（mean）、或者、差值的和（sum）

```
!!!!!!!!!L1Loss这个函数，只能进行float数据的计算，因此需要对初始的tensor进行“dtype=torch.float32”的数据转换
```

![image-20220918085823524](typora-user-images\image-20220918085823524.png)

![image-20220918085635719](typora-user-images\image-20220918085635719.png)

代码实战案例1——mean类型：

![image-20220918090323019](typora-user-images\image-20220918090323019.png)

代码实战案例2——sum类型：

![image-20220918090426994](typora-user-images\image-20220918090426994.png)

## 2 nn.MSELoss(均方误差)的说明和使用（也有mean和sum两种方式）

代码实践（输入同样需要是float类型的！）

![image-20220918090853614](typora-user-images\image-20220918090853614.png)

![image-20220918090947487](typora-user-images\image-20220918090947487.png)



## 3 nn.CrossEntropyLoss(交叉熵函数)的说明和使用

1、文档说明：

[CrossEntropyLoss — PyTorch 1.8.1 documentation](https://pytorch.org/docs/1.8.1/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)

具体的计算公式：

![image-20220918091737413](typora-user-images\image-20220918091737413.png)

如图可知，只有当命中对的类别的概率（x[class]）很大、且命中其他类别的概率很小时，loss会很小。



使用时的class参数、以及实体对象的参数：

![image-20220918092348213](typora-user-images\image-20220918092348213.png)

这里`input:(N,C)`中：

N表示batchSize、C表示类别 （因此使用交叉熵函数前要先把数据转化为（N, C）的类型）



2、代码示例

![image-20220918093835489](typora-user-images\image-20220918093835489.png)

验算得证（编码中的log就是ln）

![image-20220918093903512](typora-user-images\image-20220918093903512.png)



## 4 反向传播（grad）

1、讲解：

（lossFunction为我们反向传播提供依据）

（即更新修改的参数）

举例：梯度下降法就是利用grad不断接近正确值

![image-20220918094536088](typora-user-images\image-20220918094536088.png)



2、代码实践：

```
import torch
import torchvision
from torch import nn
from torch.nn import Conv2d, MaxPool2d, Flatten, Linear
from torch.utils.data import DataLoader

# 导入并加载数据集
dataset = torchvision.datasets.CIFAR10("./Cifar10", train=False, transform=torchvision.transforms.ToTensor(), download=True)
dataloader = DataLoader(dataset, batch_size=1) # 这里batchSize设置为1，方便之后计算交叉熵函数

# 自己设计自己的模型
class Lzp(nn.Module):
    def __init__(self): # 编写初始化函数，即各个操作类的实例化
        super(Lzp, self).__init__()
        self.sequential = nn.Sequential(
            Conv2d(in_channels=3, out_channels=32, kernel_size=5,
                   stride=1, padding=2),
            MaxPool2d(kernel_size=2),

            Conv2d(in_channels=32, out_channels=32, kernel_size=5,
                   stride=1, padding=2),
            MaxPool2d(kernel_size=2),

            Conv2d(in_channels=32, out_channels=64, kernel_size=5,
                    stride=1, padding=2),
            MaxPool2d(kernel_size=2),

            Flatten(),
            Linear(in_features=1024, out_features=64),
            Linear(in_features=64, out_features=10)
        )

    def forward(self, x): # 编写forward函数，即各个实例化的类如何真正操作
        x = self.sequential(x)
        return x

# 实例化lzp模型，并实验使用交叉熵函数
lzp = Lzp()
loss_cross = nn.CrossEntropyLoss()

for data in dataloader:
    img, target = data
    output = lzp(img) # 对图片进行卷积、池化等操作
    result_loss = loss_cross(output, target) # 计算模型返回值和target的交叉熵
    print(result_loss)
```

打印的效果如下：

![image-20220918095638596](typora-user-images\image-20220918095638596.png)



3、利用交叉熵函数返回的grad值，进行反向传播，再用debug来查看反向传播对原有参数的影响

①修改部分代码：

```
for data in dataloader:
    img, target = data
    output = lzp(img) # 对图片进行卷积、池化等操作
    result_loss = loss_cross(output, target) # 计算模型返回值和target的交叉熵
    result_loss.backward()
    print("ok")
```

②设置断点，debug（此时只运行到了44行，45行还没有运行）

![image-20220918100007977](typora-user-images\image-20220918100007977.png)

③依次点击

lzp -> sequential -> protected attributes ->  modules

就可以看到我们模型中的详细卷积/池化操作了

④随便点击某一个操作中的grad数据，点击下一步，观察这个数据是否发生了变化

![image-20220918100947807](typora-user-images\image-20220918100947807.png)

![image-20220918100607704](typora-user-images\image-20220918100607704.png)

原来grad是none，现在有值了

![image-20220918100834998](typora-user-images\image-20220918100834998.png)





# p24 优化器 （nn.Optim）



## 1 官方文档及其说明解释

[torch.optim — PyTorch 1.8.1 documentation](https://pytorch.org/docs/1.8.1/optim.html)

1、optim中有各式各样的优化器算法：

​		他们的参数各不相同（但是一般都有①模型参数	②学习率参数）

2、Optim.step方法：利用loss求出来的梯度（grad），对已有的参数进行调整

3、官网代码举例：

```
for input, target in dataset:
    optimizer.zero_grad() # 第一行把上一个循环中每个参数对应的梯度清零
    output = model(input) # 对输入数据进行模型训练
    loss = loss_fn(output, target) # 对训练结果求loss
    loss.backward() # 反向传播，利用loss对grad进行更改
    optimizer.step() # 梯度优化，通过grad对模型的参数进行调整
```

4、学习率（lr： learning rate）（不能太大也不能太小）

​			如果太小，会导致模型训练非常慢，但是loss会稳定下降

​			如果太大，会导致模型训练不稳定（loss忽大忽小），但是模型训练会很快



## 2 代码实战

1、有关优化器的代码：

```
# 实例化优化器SGD（随机梯度下降）,只设置前2个参数即可（模型参数、lr）
optim = torch.optim.SGD(lzp.parameters(), lr=0.01)

for data in dataloader:
img, target = data
output = lzp(img) # 对图片进行卷积、池化等操作
result_loss = loss_cross(output, target) # 计算模型返回值和target的交叉熵

# 梯度清零，一定要写在backward和step函数的前面！！！
optim.zero_grad()
# 反向传播，根据loss来设置grad
result_loss.backward()
# 梯度优化，根据grad来调整模型的参数
optim.step()
```

2、整体代码：

```
import torch
import torchvision
from torch import nn
from torch.nn import Conv2d, MaxPool2d, Flatten, Linear
from torch.utils.data import DataLoader

# 导入并加载数据集
dataset = torchvision.datasets.CIFAR10("./Cifar10", train=False, transform=torchvision.transforms.ToTensor(), download=True)
dataloader = DataLoader(dataset, batch_size=1) # 这里batchSize设置为1，方便之后计算交叉熵函数

# 自己设计自己的模型
class Lzp(nn.Module):
    def __init__(self): # 编写初始化函数，即各个操作类的实例化
        super(Lzp, self).__init__()
        self.sequential = nn.Sequential(
            Conv2d(in_channels=3, out_channels=32, kernel_size=5,
                   stride=1, padding=2),
            MaxPool2d(kernel_size=2),

            Conv2d(in_channels=32, out_channels=32, kernel_size=5,
                   stride=1, padding=2),
            MaxPool2d(kernel_size=2),

            Conv2d(in_channels=32, out_channels=64, kernel_size=5,
                    stride=1, padding=2),
            MaxPool2d(kernel_size=2),

            Flatten(),
            Linear(in_features=1024, out_features=64),
            Linear(in_features=64, out_features=10)
        )

    def forward(self, x): # 编写forward函数，即各个实例化的类如何真正操作
        x = self.sequential(x)
        return x

# 实例化lzp模型，并实验使用交叉熵函数
lzp = Lzp()
loss_cross = nn.CrossEntropyLoss()
# 实例化优化器SGD（随机梯度下降）,只设置前2个参数即可（模型参数、lr）
optim = torch.optim.SGD(lzp.parameters(), lr=0.01)

for i in range(20): # 让模型反复训练这个数据集，因为只让模型训练一次的话，cpu学不到什么知识，loss_result下降不明显（温故而知新）
    sum_loss = 0
    for data in dataloader:
        img, target = data
        output = lzp(img) # 对图片进行卷积、池化等操作
        result_loss = loss_cross(output, target) # 计算模型返回值和target的交叉熵

        # 梯度清零，一定要写在backward和step函数的前面！！！
        optim.zero_grad()
        # 反向传播，根据loss来设置grad
        result_loss.backward()
        # 梯度优化，根据grad来调整模型的参数
        optim.step()

        sum_loss = sum_loss + result_loss
    print(sum_loss) # 打印每一轮大循环后，loss的总和，理论上这个sumloss会越来越小
```

3、结果如下（可以看到sum_loss在逐轮下降）

![image-20220918120439181](typora-user-images\image-20220918120439181.png)

4、可以通过设置断点+debug的方式来判断

![image-20220918113405465](typora-user-images\image-20220918113405465.png)

这3句的作用

在debug的过程中依次点击

lzp -> sequential -> protected attributes ->  modules ->任意一个步骤 -> weight -> grad/data

观察grad/data这两行代码的具体变化



# p25 现有网络模型的使用与修改



1、torch.nn 是指神经网络模型

2、torchaudio 是语音相关的模型

3、torchtxt 是文字相关的模型

4、torchvision 是图像识别相关的模型



## 1 在torchvision网页中看模型的官方文档

[torchvision.models — Torchvision 0.8.1 documentation (pytorch.org)](https://pytorch.org/vision/0.8/models.html)



![image-20220918132245301](typora-user-images\image-20220918132245301.png)

其中：

classification：图像分类模型

Semantic Segmentation：语义分割

Object Detection：目标识别

Video classification：视频分类模型



## 2 VGG16模型的使用与修改

1、最常用的是VGG16模型，模型的定义如下：

![image-20220918135233277](typora-user-images\image-20220918135233277.png)

2、pretrained参数的使用：

​		如果为false，则返回一个初始化的模型即可

​		如果为true，则先创建一个初始化的模型，然后把该模型在ImageNet数据集中训练之后的所有参数覆盖到原有的初始化模型中

​								然后返回这个预训练后的模型，此时需要花费一定的模型下载时间（一个预训练好的模型下载时间也很久的）

​	（由于ImageNet太大了，160G，因此这里我们不用这个数据集进行演示）

​	（具体的数据集导入并加载的方法，可以参考官方文档torchvision.datasets: [torchvision.datasets — Torchvision 0.8.1 documentation (pytorch.org)](https://pytorch.org/vision/0.8/datasets.html)）

​	   (但是ImageNet已经无法公开访问了，不能通过控制台输入代码来下载，只能去官网手动下载！)



3、具体调用代码：

```
import torchvision

vgg16_false = torchvision.models.vgg16(pretrained=False) # 返回初始模型，不需要预训练
vgg16_true = torchvision.models.vgg16(pretrained=True) # 返回预训练模型

print(vgg16_true) # 打印预训练的模型结构
```

结果如下：（注意预训练的模型的下载路径在c盘）

![image-20220918140924377](typora-user-images\image-20220918140924377.png)

打印的预训练模型的结构如下：

```
D:\anaconda\envs\lzp\python.exe D:/pycharm/test/p25_VGG16/p25_VGG16.py 
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)

Process finished with exit code 0
```

### （1） 修改vgg模型，使用到CIFAIR10数据集中

1、从ImageNet数据集中可知，它是一个1000个分类的图片分类数据集，因此

​	预训练好的vgg模型最后也进行了4096 -> 1000的线性变化，最后输出了1000个类



​	但是此刻我们想要把这个vgg模型应用于CIFAIR10（这个数据集只有10个类别的图片）数据集中

​	那么我们就需要修改vgg16模型，让它最后的输出只有10个类！



2、修改模型的方法有两种：

#### ① 直接修改最后一个线性层（4096->1000更改为4096->10）（比较暴力，不推荐）

首先定位要修改的操作的位置（在classifier层的6号位置）

![image-20220918144051249](typora-user-images\image-20220918144051249.png)

编写修改代码：

```
vgg16_true.classifier[6] = nn.Linear(in_features=4096, out_features=10)
```

结果如下：

![image-20220918144125334](typora-user-images\image-20220918144125334.png)





#### ② 在模型最后，再添加一个线性层（1000->10）

使用`add_module`方法：

（这个方法，只会对当前py文件中的模型做改动，改动不是永久的，下一次运行时，又会回到初始状态）

（因此在下一次修改时，不必删除添加的语句）

```
vgg16_true.add_module("添加的操作的名称", 具体操作内容)
举例：
vgg16_true.add_module("add_linear", nn.Linear(in_features=1000, out_features=10))
```

结果如下：

![image-20220918143241675](typora-user-images\image-20220918143241675.png)



而如果想要把这句操作添加到这个模型的classifier这个层次中去：

```
vgg16_true.classifier.add_module("add_linear", nn.Linear(in_features=1000, out_features=10))
```

结果如下：

![image-20220918143718617](typora-user-images\image-20220918143718617.png)



整体完整代码：

```
import torchvision
from torch import nn

vgg16_false = torchvision.models.vgg16(pretrained=False) # 返回初始模型，不需要预训练
vgg16_true = torchvision.models.vgg16(pretrained=True) # 返回预训练模型

print(vgg16_true) # 打印预训练的模型结构

# 导入数据集
dataset = torchvision.datasets.CIFAR10("./Cifar10", train=True, transform=torchvision.transforms.ToTensor(),
                                       download=True)

# 在预训练的模型后面，添加一个linear方法（1000->10）
# vgg16_true.classifier.add_module("add_linear", nn.Linear(in_features=1000, out_features=10))
# print(vgg16_true)

# 直接修改预训练模型的最后一个linear操作
vgg16_true.classifier[6] = nn.Linear(in_features=4096, out_features=10)
print(vgg16_true)
```



# p26 网络模型的保存与读取

由p26的内容，讲到过：

一个py文件对模型的修改，只会对当前py文件中的模型做改动，改动不是永久的，下一次运行时，又会回到初始状态



如果我们需要保存我们修改之后的模型文件，那么就需要在修改完的代码后边添加模型的保存代码，在重新使用时编写读取代码



## 方法一 直接保存模型+参数，且直接读取模型

模型文件默认类型为`.pth`，且默认保存在当前py文件的文件夹中

```
torch.save(保存的模型对象, 保存的路径名称文件)
举例：
torch.save(vgg16_false, "vgg16_save_path.pth")
```

具体实践的保存代码：

```
import torch
import torchvision
from torch import nn

# 创建一个初始模型
vgg16_false = torchvision.models.vgg16(pretrained=False)
# 直接修改预训练模型的最后一个linear操作
vgg16_false.classifier[6] = nn.Linear(in_features=4096, out_features=10)

# 保存方式1
torch.save(vgg16_false, "vgg16_save_path.pth")
```

结果如下：

![image-20220918155156750](typora-user-images\image-20220918155156750.png)

具体实践的加载代码：

```
model = torch.load("模型文件的路径")
举例：
model = torch.load("./vgg16_save_path.pth")
```

完整代码：

```
import torch

# 使用方法1 加载模型
model = torch.load("./vgg16_save_path.pth")
print(model)
```



## 方法二 （官方推荐）只保存模型的参数（保存到dict类型的数据结构中），且在新的模型中加载参数

1、保存的代码模版：

```
torch.save(模型对象.state_dict(), 模型保存pth类型文件的名称)
举例：
torch.save(vgg16_false.state_dict(), "vgg16_save_path2.pth")
```

完整代码：

```
import torch
import torchvision
from torch import nn

# 创建一个初始模型
vgg16_false = torchvision.models.vgg16(pretrained=False)
# 直接修改预训练模型的最后一个linear操作
vgg16_false.classifier[6] = nn.Linear(in_features=4096, out_features=10)

# 保存方式2
torch.save(vgg16_false.state_dict(), "vgg16_save_path2.pth")
```

结果如下：

![image-20220918160113011](typora-user-images\image-20220918160113011.png)

2、加载的代码模版

```
# 使用方法2 加载模型
# 第一步：先获取初始模型
model = torchvision.models.vgg16(pretrained=False)
# 第二步：获取保存好的所有模型参数（dict类型）
my_dict = torch.load("./vgg16_save_path2.pth")
# 第三部：把参数覆盖到原始模型中
model.load_state_dict(my_dict)
print(model)
```

也可以得到结果：

但是！！！如果保存后的模型轻易修改了，那么再加载时会报错！（原因不知）



## 两种方法的差距

1、首先，可以通过`dir`命令来观察两个pth文件的大小区别：

![image-20220918160610673](typora-user-images\image-20220918160610673.png)

可见方法一保存的文件更大



2、方法一在加载前，必须要有class类定义，否则无法运行（或者模型的类被import进来）

错误示例：

![image-20220918162241656](typora-user-images\image-20220918162241656.png)

解决方法1：加载之前重新再定义一遍这个类

![image-20220918162335157](typora-user-images\image-20220918162335157.png)

解决方法2：加载之前导入这个类的包（import）

![image-20220918162437337](typora-user-images\image-20220918162437337.png)